---
title: "Camille Text Mining"
author: "Wuji Shan"
date: "11/29/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidyr)
library(httr)
library(tnum)
library(tidytext)
library(gutenbergr)
library(textdata)
library(janeaustenr)
library(dplyr)
library(stringr)
library(scales)
library(ggplot2)
library(wordcloud)
library(reshape2)
```

Book: Camille (LA DAME AUX CAMILIAS)  
Author: Alexandre Dumas  

# Task One
```{r}
Cami <- gutenberg_download(1608)
```

```{r}
newCami <- Cami %>%
  mutate(linenumber = row_number()) %>%
  select(-gutenberg_id) %>%
  mutate(chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE))))
```

```{r}
tidy_Cami <- newCami %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

tidy_Cami %>%
  count(word, sort = TRUE) 
```

# Task Two: Sentiment Analysis

## nrc

```{r}
#textdata::lexicon_nrc(delete = TRUE)
#nrc <- textdata::lexicon_nrc()
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_Cami %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

## bing

```{r}
get_sentiments("bing")
bing_neg <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")
```

```{r}
tidy_Cami %>%
  inner_join(bing_neg) %>%
  count(word, sort = TRUE)

Cami_sentiment <- tidy_Cami %>%
  inner_join(get_sentiments("bing")) %>%
  count(index = chapter, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

ggplot(Cami_sentiment, aes(index, sentiment)) +
  geom_col(show.legend = FALSE)
```

## afinn

```{r}
afinn <- tidy_Cami %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")
```

## compare the three sentiment dictionaries

```{r}
bing_and_nrc <- bind_rows(
  tidy_Cami %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  tidy_Cami %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

## later

```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)
```

```{r}
get_sentiments("bing") %>% 
  count(sentiment)
```

## Most common positive and negative words
```{r}
bing_word_counts <- tidy_Cami %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts
```

```{r}
bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```

## Wordclouds

```{r}
tidy_Cami %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))
```

```{r}
tidy_Cami %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

Find the number of negative words in each chapter and divide by the total words in each chapter. 
Which chapter has the highest proportion of negative words?
```{r}
wordcounts <- tidy_Cami %>%
  group_by(chapter) %>%
  summarize(words = n())

tidy_Cami %>%
  semi_join(bing_neg) %>%
  group_by(chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()
```
Chapter 24 has the highest proportion of negative words.
